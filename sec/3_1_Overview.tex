\subsection{Overview}

The overall framework of this paper is shown in \cref{fig:framework}. First, this method requires the input of the target image $x_t$ (real angiography). The vascular structure in the target image $x_t$ is removed by Fourier transform to generate a background image $x_b$. Then the renderer uses volume rendering technology to render the 3D vascular object into the background image to obtain an artificially synthesized angiography $x_s$. In this process, the renderer can also adjust the concentration of contrast agent in the 3D blood vessels (the transparency of the 3D vascular object) to obtain a perturbed image $x_d$ with a different contrast agent concentration from the angiography $x_s$. In addition, the actual mask $\text{mask}_s$ of the synthesized image can be obtained by projecting the 3D vascular object onto a 2D plane. This paper adopts a student-teacher architecture, in which the student module and the teacher model use the same U-Net backbone network. Both the unlabeled target image $x_t$ and the synthesized source image $x_s$ are analyzed using the student network to obtain the feature $f_t$ of a single pixel of the target image and the feature $f_s$ of a single pixel of the source image, respectively. The perturbed image $x_d$ obtains the semantic features $f_d$ of the corresponding single pixel of the perturbed image through the teacher network. The semantic features of each pixel are fully connected and softmaxed to obtain the blood vessel-background score of the pixel. Pixels with a score higher than 0.5 are predicted as blood vessels, otherwise they are predicted as background. By scoring and predicting the target image features and the source image features, the prediction mask $y_t$ of the real image and the prediction mask $y_s$ of the synthetic image can be obtained respectively.

In the architecture of this article, there are three modules that need parameter optimization. The student module optimizes parameters through the stochastic gradient descent method. 
The loss function used in this process is the weighted sum of segmentation loss, contrast loss, adversarial loss, and consistency loss:
\begin{equation}
  \text{Loss} = L_{\text{seg}} + \lambda_1 L_{\text{cont}} + \lambda_2 L_{\text{adv}} + \lambda_3 L_{\text{cons}}
\end{equation}
  
The teacher module is updated synchronously with the student parameters based on the EMA algorithm. 
The discriminator network used for adversarial learning is updated based on the difference between the predicted mask $y_t$ of the real image and the predicted mask $y_s$ of the synthetic image.


% 插入单栏插图
\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{pic/2.png}
  \caption{The framework of the proposed method}
  \label{fig:framework}
\end{figure*}

%-------------------------------------------------------------------------

\subsubsection{Segmentation loss for synthetic images}
The segmentation loss of the synthetic image measures the difference between the predicted mask $y_s$ of the synthetic image and the actual mask $\text{mask}_s$ of the synthetic image. The segmentation loss of the synthetic image is used to learn the perspective projection law of tubular tree objects from artificially synthesized labeled data. The specific form of the segmentation loss is the weighted sum of the Dice loss and the cross entropy loss:
\begin{equation}
L_{\text{seg}} = \text{Dice} + \lambda_0 \text{CE}
\end{equation}

%-------------------------------------------------------------------------

\subsubsection{Consistency Regularization}
The consistency regularization loss measures the difference between the semantic features of the source image and the semantic features of the perturbation image. Perturbation consistency is used to let the neural network learn that ``contrast agent concentration does not affect the geometric structure of blood vessels'', so that the framework has better robustness. The specific form of this consistency regularization loss is cosine loss:
\begin{equation}
L_{\text{cons}} = \mathbb{E} \left[ \left( 1 - \frac{f_s \cdot f_d}{\|f_s\| \cdot \|f_d\|} \right) / 2 \right]
\end{equation}

%-------------------------------------------------------------------------

\subsubsection{Adversarial Learning}
As shown in \cref{fig:Adversarial}, the discriminator network can determine whether the input mask is the predicted mask $y_t$ of the real image or the predicted mask $y_s$ of the synthetic image. This ability to analyze the difference between the two blood vessel contours is obtained through training based on the discriminator loss. The specific form of the discriminator loss is binary cross entropy:
\begin{equation}
L_{\text{disc}} = \mathbb{E}[-\log(D(y_s))] + \mathbb{E}[-\log(1 - D(y_t))]
\end{equation}

Adversarial loss is used to make the predicted masks of real images generated by the student network as indistinguishable as possible from the predicted masks of synthetic images. Therefore, adversarial learning can enable the student network to transfer the knowledge learned from synthetic images to real images. The specific form of this adversarial loss is information entropy:
\begin{equation}
L_{\text{adv}} = \mathbb{E}[-\log(D(y_t))]
\end{equation}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{pic/3.adv.png}
  \caption{The framework of the proposed method}
  \label{fig:Adversarial}
\end{figure}

%-------------------------------------------------------------------------

\subsubsection{Contrastive Learning}
The contrastive learning loss measures the difference between the target image features and the source image features. The contrast is used to let the student neural network learn that ``the blood vessels in the real target image are also 3D tubular tree structures like the blood vessels in the synthetic source image", so as to transfer the knowledge learned from the synthetic image to the real image. The specific form of this contrastive learning loss is info-NCE:
\begin{equation}
  L_{\text{cont}}
  %L_{\text{cont}} = \mathbb{E} \left[ -\log \frac{\exp(f_t^+ \cdot f_s^+ / \tau)}{\exp(f_t^+ \cdot f_s^+ / \tau) + \sum_{j \neq t} \exp(f_t^+ \cdot f_s^j / \tau) + \sum_{j \neq s} \exp(f_s^+ \cdot f_t^j / \tau)} \right]
\end{equation}

As shown in \cref{fig:Contrastive}, contrastive learning first distinguishes whether each pixel in the image represents a blood vessel or background based on the predicted mask $y_t$ of the real image and the predicted mask $y_s$ of the synthetic image. Then, contrastive learning loss is used to shorten the distance between the blood vessel features $f_t^+$ in the target image and the blood vessel features $f_s^+$ in the source image. At the same time, the distance between the blood vessel features $f_s^-$ in the source image and the background features $f_t^-$ in the target image is further away.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.4\textwidth]{pic/4.cont.png}
  \caption{The framework of the proposed method}
  \label{fig:Contrastive}
\end{figure}

%-------------------------------------------------------------------------
